{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "**Web Scraping** is the process of crawling across a webpage and collecting the text content. Some popular services, such as SkyScanner, collect data from many sources and display it all together for their users to compare. \n",
    "\n",
    "Web scraping is often considered an option if there is no API, but beware that it lies in a grey area in terms of ethics and legality. You should not scrape data that isn't publicly available, and many websites have implemented protection measures to prevent web scrapers from taking their data. But in many cases, it is a perfectly acceptable data collection method, especially as a learning exercise. \n",
    "\n",
    "If you want to check a website's policy on scraping, most websites have a file called [`robots.txt`](https://www.promptcloud.com/blog/how-to-read-and-respect-robots-file/), which will be the base URL + \"/robots.txt\". This will contain rules to communicate to bots crawling the site what they should and shouldn't do. \n",
    "\n",
    "## HTML\n",
    "\n",
    "The main content of a webpage is displayed in **HTML** (HyperText Markup Language) files. Web browsers format the HTML elements, but additional styling and functionality can be added with **CSS** (Cascading Style Sheet) and **JavaScript**. The code for CSS and JavaScript are commonly written in separate files and imported into the HTML to save space, but not always! \n",
    "\n",
    "We can use the inspector tool in a web browser to look at the HTML for any webpage. You could also consider using a tool like [Live DOM Viewer](https://software.hixie.ch/utilities/js/live-dom-viewer/). **DOM** = Document Object Model, which is the tree of elements that make up an HTML document.\n",
    "\n",
    "If you want to create your own `.html` file and experiment:\n",
    "  - Create a file called `index.html`, and boilerplate an HTML document with `!`\n",
    "  - `<head>` tag holds metadata, `<body>` tag holds elements that render on the page\n",
    "  <!-- - Add `<style>` tag to demonstrate CSS (and also how a separate file could be linked)\n",
    "  - Add `<script>` tag to demonstrate JS (and also how a separate file could be linked) -->\n",
    "  - Explore some basic tags and common attributes (`<h1>`, `<p>`, `<a>`, `<img>`)\n",
    "  - Explore HTML tag nesting (`<div>`, `<ul>`)\n",
    "\n",
    "Now that we know what we're looking for, we can look at some Python libraries to help us achieve it! Choosing which library will depend on the website you are looking at, and how the page content is generated and displayed. \n",
    "\n",
    "There are two main types of webpages you'll find online: **static** and **dynamic**. Static refers to HTML pages that are \"finished\" on arrival, all the content is already available when the document is received. Dynamic pages use JavaScript to build the HTML _after_ the browser receives the document, meaning there will be a short (often barely even perceptible to humans) time in which the page doesn't have the content. \n",
    "\n",
    "There is also the question of accessing the exact data you are after - will you need to interact with elements on the webpage to navigate, filter, search, etc? \n",
    "\n",
    "## Beautiful Soup\n",
    "\n",
    "[**Beautiful Soup**](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is our recommended library for static sites. It is a lightweight package designed for parsing HTML and XML files. You should add `beautifulsoup4` (bs4) to your Conda environment for this project. You will also need to add [`requests`](https://pypi.org/project/requests/) to make the HTTP requests, and (optional) [`lxml`](https://lxml.de/) to help process the HTML/XML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "response = requests.get(\"https://example.com/\")\n",
    "# soup = bs4.BeautifulSoup(response.text, 'lxml') # with lxml package\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser') \n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have some \"soupified\" HTML, we can target specific elements inside and collect the data content they are holding. There are actually many maaaany ways we can do this (refer to documentation), and we should specify if we want a single or multiple results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target first <p> element inside the body:\n",
    "soup.body.p\n",
    "\n",
    "# target first <p> element in the document:\n",
    "soup.find('p')\n",
    "soup.select_one('p')\n",
    "\n",
    "# # target all <p> elements in the document:\n",
    "soup.find_all('p')\n",
    "soup.select('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select` and `select_one` methods will accept [CSS Selectors](https://www.freecodecamp.org/news/css-selectors-cheat-sheet/), while `find` and `find_all` will take [multiple arguments](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find_all), including the tag name and attributes (if you ever want to practise CSS selectors, [here](https://flukeout.github.io/) is a great resource!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these both target <p class=\"red\">this is a paragraph</p>\n",
    "soup.find_all(\"p\", attrs={\"class\": \"red\"})\n",
    "soup.select(\"p.red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have selected an element, you can access the [attributes](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#bs4.Tag.attrs) and [text](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text), and you can also [navigate](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree) to its surrounding elements (children, parents, siblings) through the DOM tree. \n",
    "\n",
    "The website [books.toscrape.com](https://books.toscrape.com/) is a dummy website build for the purpose of practising web scraping. We can build a small dataset by taking some of the content. I want to make a dataframe from the books, collecting their title, rating, price, and the url for the cover image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://books.toscrape.com/\")\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the HTML, I can see that the cards holding the data for each book are `<li>` (list) elements nested inside an `<ol>` (ordered list) element with a class of \"row\", and that the elements holding the content I want are grouped into an `<article>` element with a class of \"product_pod\". It is important to make sure I am not going to end up with any other rogue elements in my list of books, as this could break my code. The more specific the combination, the less likely the chance of overlapping selectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the selector down to show nesting\n",
    "# book_cards = soup.select(\"article.product_pod\")\n",
    "book_cards = soup.find_all(\"article\", attrs={\"class\": \"product_pod\"})\n",
    "book_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of the article elements, which I can loop over and extract the data I want. To get the title, I'll need to look at the `<a>` inside the `<h3>` (there is more than one `<a>` element in the card, so I have to be specific when selecting). You'll notice though that the text cuts off the end of the title if it's too long, so we want to take the value from the `title` attribute, rather than the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "for book in book_cards:\n",
    "  # title = book.select_one('h3 > a')['title']\n",
    "  title = book.find('h3').findChild('a')['title']\n",
    "  titles.append(title)\n",
    "\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the the rating, I need to understand their system. It seems they are using a class of \"One\" to \"Five\" to determine how the stars display, so the second class on the `<p>` element with a first class of \"star-rating\" is what tells me the rating. The class attribute will be a list of all classes applied to the element, and since I only want the second, I will use the index. At this point, I could also write a simple function to convert the string to an Int for my data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "\n",
    "for book in book_cards:\n",
    "  # rating = book.select_one('.star-rating')['class'][1]\n",
    "  rating = book.find(attrs={'class': 'star-rating'})['class'][1]\n",
    "  ratings.append(rating)\n",
    "  \n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rating(string):\n",
    "  return [\"One\", \"Two\", \"Three\", \"Four\", \"Five\"].index(string) + 1\n",
    "\n",
    "ratings_int = [convert_rating(string) for string in ratings]\n",
    "ratings_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the price, I can target the `<p>` element with a class of \"price_color\", and extract the text. At this point I could also convert this to a float (first remove the symbols). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = []\n",
    "\n",
    "for book in book_cards:\n",
    "  price = book.find(\"p\", attrs={\"price_color\"}).get_text()\n",
    "  formatted_price = float(price.replace('Â£', ''))\n",
    "  prices.append(formatted_price)\n",
    "  \n",
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to access the cover image URL, I will need to target the `<img>` tag and extract it from the `src` attribute. You'll notice though that it isn't a complete URL. This is because the images are hosted on the same domain, so we can create a full URL that can be linked from anywhere by adding the website's base URL to the string we get from the image source (always double check your `/` symbols when concatenating URL strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covers = []\n",
    "\n",
    "base_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "for book in book_cards:\n",
    "  cover_extension = book.find(\"img\")[\"src\"]\n",
    "  covers.append(base_url + cover_extension)\n",
    "  \n",
    "covers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have my lists, I can create a DataFrame and [CSV](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "books_df = pd.DataFrame({\n",
    "  'title': titles,\n",
    "  'price': prices,\n",
    "  'rating': ratings,\n",
    "  'cover': covers\n",
    "})\n",
    "\n",
    "books_df.to_csv(\"out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has only collected data from the _first_ page though. If we look at the website, we can navigate to page 2 with the click of a button. Notice the URL indicates which page we are on - this is a great opportunity to loop and gather not just the first 50, but all 1000 books listed on the website! This site is nice enough to indicate how many pages we have to iterate over, but many websites won't give you this. It is also likely to change as stock levels rise and drop! So we have to come up with a dynamic solution. \n",
    "\n",
    "If the total page number is displayed on the website, you can scrape that value and `for` loop over a [range](https://www.w3schools.com/python/ref_func_range.asp). If it isn't displayed, then we're going to need to use a while loop until there are no more pages. If we open [page 51](https://books.toscrape.com/catalogue/page-51.html), we get an error 404 page. This error status code will be accessible in the response object returned by `requests.get()`, which we can set to the condition of the loop.\n",
    "\n",
    "We can format the URL string using Python's [string_format](https://www.w3schools.com/python/ref_string_format.asp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number = 1\n",
    "while True:\n",
    "  url = f\"{base_url}catalogue/page-{page_number}.html\"\n",
    "  response = requests.get(url)\n",
    "  print(page_number, response)\n",
    "  page_number += 1\n",
    "  if response.status_code != 200:\n",
    "    break\n",
    "  # here would be the rest of my scraping code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that Beautiful Soup is only going to work for static webpages. If we try to get the HTML for a dynamic page (take the LMS for example, which is a React App, meaning the page content is generated in the browser when you open it), you'll get a mostly empty page, and some JavaScript nonsense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://lms.codeacademyberlin.com/\")\n",
    "soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task! \n",
    "\n",
    "You will use BeautifulSoup to scrape data from the [Financial Times](https://www.ft.com/). Use the search feature to find results specific to your chosen cryptocurrency. You can also add additional filters, notice how they are added to the URL! The purpose of this exercise is to collect data, don't worry when the data you're collecting is not the most relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAB_Module3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
